{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"newtrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "svd_model = TruncatedSVD(n_components=100, algorithm='randomized', n_iter=5)\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer),\n",
    "                            ('svd', svd_model)])\n",
    "\n",
    "g = itertools.chain(data.question1, data.question2)\n",
    "svd_transformer.fit(g)\n",
    "\n",
    "q1, q2 = svd_transformer.transform(data.question1), svd_transformer.transform(data.question2)\n",
    "# Memory inefficient\n",
    "data['cosine_distance'] = np.diag(cosine_similarity(q1, q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = data.pop('is_duplicate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    encoding='utf-8',\n",
    "    analyzer='word',\n",
    "    strip_accents='unicode',\n",
    "    ngram_range=(1, 1),\n",
    "    lowercase=True,\n",
    "    norm='l2',\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True,)\n",
    "\n",
    "cosine_vals = []\n",
    "\n",
    "for i in data.id:\n",
    "    try:\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform([data.loc[i]['question1'], data.loc[i]['question2']])\n",
    "        cosine_vals.append(cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)[0][1])\n",
    "    except:\n",
    "        cosine_vals.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1239223795194326\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "print(log_loss(dup, cosine_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6549011847930941\n",
      "0.6624131192955551\n",
      "0.6644067377377625\n",
      "0.6643127458012813\n",
      "0.6628064013455688\n",
      "0.6609315095599694\n",
      "0.661567191867224\n",
      "0.6652897672462836\n",
      "0.667120136535655\n",
      "0.6665413440846917\n",
      "0.6667021197655149\n"
     ]
    }
   ],
   "source": [
    "threshold = [0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9]\n",
    "for thresh in threshold:\n",
    "    y_pred = []\n",
    "    for cosine in cosine_vals:\n",
    "        if cosine>thresh:\n",
    "            cosine = 1\n",
    "        else:\n",
    "            cosine = 0\n",
    "        y_pred.append(cosine)\n",
    "\n",
    "    print(accuracy_score(dup, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report - Using tfidf model to identify duplicate question pairs\n",
    "\n",
    "Implementation\n",
    "\n",
    "    TTfidf metrix helps us to transform words into numbers, and in our case each question will be represented by a vector. Then we can decide duplication by computing vectors, that is to see how large the difference between two vectors. Our initial idea is to generate tfidf metrix by transforming the whole question1/question2 into a metrix. Then we can use SVD to select most imporatnt features and at the same time reduce dimensionality. However, we encountered memory error when trying to fit_transform the data because the metrix is huge. (See the image below for code)\n",
    "\n",
    "    Then we decided to build a metrix for each question pair one by one, so that in this way we won't encounter memory error. Then we compare the similarity of question1 and question2 by measuring the difference of vector using cosine similarity. So in the end it returned probability of duplicate for each question pair.\n",
    "\n",
    "\n",
    "Result\n",
    "\n",
    "    Because the model give us values in terms of probabilities,it is suitable to use loss log to measure the uncertainty of the probability of model by comparing them to the true labels.(Reference(3)) The log loss is 1.1265146865556896.\n",
    "    \n",
    "    Then we want to see how accurate the model is. The question is how to set the threshold for partitioning data into class 0 and class 1. We set the threshhold by hand, and the value is always around 0.667.\n",
    "    \n",
    "    This tfidf model is very basic, so we don't expect that it will perform extremely well. \n",
    "\n",
    "\n",
    "Reference\n",
    "\n",
    "    (1) http://sergeiturukin.com/2017/04/07/kaggle-quora-part1.html\n",
    "    \n",
    "    (2) https://github.com/ab-bh/Quora-Duplicate-Question-Pairs/blob/master/TF-IDF%20Approach%20.ipynb\n",
    "    \n",
    "    (3) https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function\n",
    "    \n",
    "    (4) https://github.com/YuriyGuts/kaggle-quora-question-pairs/blob/master/notebooks/feature-tfidf.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "![title](/memoryError.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hanying Li\\\\Desktop\\\\quora-question-pairs'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
