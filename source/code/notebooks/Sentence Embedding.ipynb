{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import gensim.models.word2vec\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from math import log\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "path = lambda x : '~/.kaggle/competitions/quora-question-pairs/' + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(path('train.csv'))\n",
    "cleaned_train = pd.read_csv('~/PycharmProjects/quora-question-pairs/source/data/newtrain.csv')\n",
    "cleaned_train['question1'] = cleaned_train['question1'].apply(literal_eval)\n",
    "cleaned_train['question2'] = cleaned_train['question2'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.dropna(subset=['question1', 'question2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "def my_tokenize(s, stopword=stopword):\n",
    "    s = s.lower() # put into lower case form\n",
    "    s = re.sub(r\"[\\'\\\"\\?.()/:,]\", ' ',s) # remove certain punctuations and replace it with space\n",
    "    s = re.sub(r\"\\s+\",' ',s) # remove extra space\n",
    "    s = re.sub(r\"not\\s+\",'not_', s) # deal with negation\n",
    "    tokens = word_tokenize(s) # split string into words (tokens)\n",
    "    porter = PorterStemmer() \n",
    "    tokens = [porter.stem(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in set(stopword)] # remove stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    stopword = stopwords.words('english')\n",
    "    stopword = [e for e in stopword if e not in ('what', 'when', 'where','whether','which','whose', 'why','should','how')]\n",
    "    contractions = json.load(open('../data/contractions.json', 'r+'))\n",
    "    # spread out contraction for question1 and question2\n",
    "    question1 = []\n",
    "    for s1 in df['question1']:\n",
    "        tokens = re.findall(r'\\S+', s1)\n",
    "        mylist= []\n",
    "        for t in tokens:\n",
    "            if t in contractions:\n",
    "                t = contractions[t]\n",
    "            mylist.append(t)\n",
    "            result1 = \" \".join(mylist)\n",
    "        question1.append(result1)\n",
    "       \n",
    "\n",
    "\n",
    "    question2 = []\n",
    "    for s2 in df['question2']:\n",
    "        tokens = re.findall(r'\\S+', s2)\n",
    "        mylist= []\n",
    "        for t in tokens:\n",
    "            if t in contractions:\n",
    "                t = contractions[t]\n",
    "            mylist.append(t)\n",
    "            result2 = \" \".join(mylist)\n",
    "        question2.append(result2)\n",
    "\n",
    "    myquestion1 = []\n",
    "    myquestion2 = []\n",
    "    for s1, s2 in zip(df['question1'], df['question2']):\n",
    "        myresult1 = my_tokenize(s1, stopword)\n",
    "        myresult2 = my_tokenize(s2, stopword)\n",
    "        myquestion1.append(myresult1)\n",
    "        myquestion2.append(myresult2)\n",
    "    df['question1']= myquestion1\n",
    "    df['question2']= myquestion2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = clean_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load('/Users/John/PycharmProjects/quora-question-pairs/source/data/gensimModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gensim_load_vec(path=\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"):\n",
    "    #use gensim_emb.wv.index2word if used this way to load vectors\n",
    "    #gensim_emb = gensim.models.word2vec.Word2Vec.load(path)\n",
    "    gensim_emb =  gensim.models.KeyedVectors.load_word2vec_format(path, binary=True,limit=500000)\n",
    "    vocab = gensim_emb.index2word\n",
    "    shape = gensim_emb.syn0.shape\n",
    "    return gensim_emb, vec, shape, vocab\n",
    "\n",
    "def map_word_frequency(document):\n",
    "    return Counter(itertools.chain(*document))\n",
    "\n",
    "def sentence2vec(tokenised_sentence_list, embedding_size, word_emb_model, a = 1e-3):\n",
    "    sentence_vecs = []\n",
    "    sentence_set=[]\n",
    "    word_counts = map_word_frequency(tokenised_sentence_list)\n",
    "    for sentence in tokenised_sentence_list:\n",
    "        vs = np.zeros(embedding_size)\n",
    "        sentence_length = len(sentence)\n",
    "        for word in sentence:\n",
    "            a_value = a / (a + word_counts[word]) # smooth inverse frequency, SIF\n",
    "        try:\n",
    "            vs = np.add(vs, np.multiply(a_value, word_emb_model[word])) # vs += sif * word_vector\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        #vs = np.log(vs) - np.log(sentence_length)\n",
    "        vs = np.divide(vs, sentence_length) # weighted average\n",
    "        sentence_set.append(vs)\n",
    "    return sentence_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gensim_emb, vec, shape, vocab = gensim_load_vec()\n",
    "gensim_emb.save('/Users/John/PycharmProjects/quora-question-pairs/source/data/gensimModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_list=[]\n",
    "for q1, q2 in zip(cleaned_train['question1'], cleaned_train['question2']):\n",
    "    try:\n",
    "        token1 = word_tokenize(\" \".join(q1))\n",
    "        token2 = word_tokenize(\" \".join(q2))\n",
    "        sent_list.append(token1)\n",
    "        sent_list.append(token2)\n",
    "    except Exception as e:\n",
    "        print(q1)\n",
    "        print(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/John/Dropbox/ECS171/HW3/venv/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/John/Dropbox/ECS171/HW3/venv/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "sent_emb = sentence2vec(sent_list, 300 , model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame({'id' : [], 'is_duplicate': []})\n",
    "\n",
    "is_dup = []\n",
    "for i in range(0, 1000, 2):\n",
    "    try:\n",
    "        is_dup.append(float(cosine_similarity([sent_emb[i]],[sent_emb[i + 1]])))\n",
    "    except Exception as e:\n",
    "        print([sent_emb[i]])\n",
    "        print([sent_emb[i]])\n",
    "        print(cosine_similarity([sent_emb[i]],[sent_emb[i + 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808580"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[what, step, step, guid, invest, share, market...</td>\n",
       "      <td>[what, step, step, guid, invest, share, market]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[what, stori, kohinoor, koh-i-noor, diamond]</td>\n",
       "      <td>[what, would, happen, indian, govern, stole, k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>[how, increas, speed, internet, connect, use, ...</td>\n",
       "      <td>[how, internet, speed, increas, hack, dn]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>[whi, mental, veri, lone, how, solv]</td>\n",
       "      <td>[find, remaind, when, [, math, ], 23^, {, 24, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>[which, one, dissolv, water, quikli, sugar, sa...</td>\n",
       "      <td>[which, fish, would, surviv, salt, water]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  [what, step, step, guid, invest, share, market...   \n",
       "1   1     3     4       [what, stori, kohinoor, koh-i-noor, diamond]   \n",
       "2   2     5     6  [how, increas, speed, internet, connect, use, ...   \n",
       "3   3     7     8               [whi, mental, veri, lone, how, solv]   \n",
       "4   4     9    10  [which, one, dissolv, water, quikli, sugar, sa...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0    [what, step, step, guid, invest, share, market]             0  \n",
       "1  [what, would, happen, indian, govern, stole, k...             0  \n",
       "2          [how, internet, speed, increas, hack, dn]             0  \n",
       "3  [find, remaind, when, [, math, ], 23^, {, 24, ...             0  \n",
       "4          [which, fish, would, surviv, salt, water]             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9a9fb41db7b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstopword_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'what'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'when'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'where'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'whether'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'which'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'whose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'why'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'should'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'how'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstopword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopword_exceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/ECS171/HW3/venv/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2508\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2509\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2510\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-b02f762284f0>\u001b[0m in \u001b[0;36mmy_tokenize\u001b[0;34m(s, stopword)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# cleaning data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmy_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# put into lower case form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[\\'\\\"\\?.()/:,]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# remove certain punctuations and replace it with space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\s+\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# remove extra space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "stopword = set(stopwords.words('english'))\n",
    "stopword_exceptions = set(['what', 'when', 'where', 'whether', 'which', 'whose', 'why', 'should', 'how'])\n",
    "stopword = stopword.difference(stopword_exceptions)\n",
    "train1['question1'].apply(my_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
